@article{MacLellan2013,
abstract = {Simulated learner systems are used for many purposes ranging from computational models of learning to teachable agents. To support these varying applications, some simulated learner systems have relied heavily on machine learning to achieve the necessary generality. However, these efforts have resulted in simulated learners that sometimes make generalization errors that the humans they model never make. In this paper, we discuss an approach to reducing these kinds of generalization errors by having the simulated learner system reflect before acting. During these reflections, the system uses background knowledge to recognize implausible actions as incorrect without having to receive external feedback. The result of this metacognitive approach is a system that avoids implausible errors and requires less instruction. We discuss this approach in the context of SimStudent, a computational model of human learning that acquires a production rule model from demonstrations.},
author = {MacLellan, Christopher J. and Matsuda, Noboru and Koedinger, Kenneth R.},
file = {:C$\backslash$:/Users/dorot/Desktop/Readings/AL/Toward a reflective SimStudent{\_}Using experience to avoid generalization errors.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {Cognitive modeling,Generalization error,Grammar induction,Metacognition,Representation learning,Simulated learners},
pages = {51--60},
title = {{Toward a reflective SimStudent: Using experience to avoid generalization errors}},
volume = {1009},
year = {2013}
}
@article{Koedinger2015,
abstract = {We discuss methods for evaluating simulated learners associated with four different scientific and practical goals for simulated learners. These goals are to develop a precise theory of learning, to provide a formative test of alternative instructional approaches, to automate authoring of intelligent tutoring systems, and to use as a teachable agent for students to learn by teaching. For each goal, we discuss methods for evaluating how well a simulated learner achieves that goal. We use SimStudent, a simulated learner theory and software architecture, to illustrate these evaluation methods. We describe, for example, how SimStudent has been evaluated as a theory of student learning by comparing, across four domains, the cognitive models it learns to the hand-authored models. The SimStudent-acquired models generally yield more accurate predictions of student data. We suggest future research into directly evaluating simulated learner predictions of the process of student learning.},
author = {Koedinger, Kenneth R. and Matsuda, Noboru and Maclellan, Christopher J. and McLaughlin, Elizabeth A.},
file = {:C$\backslash$:/Users/dorot/Desktop/Readings/AL/Models for Evaluating Simulated Learners{\_}Examples from SimStudent.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {Cognitive models,Instructional theory,Learning theory},
pages = {45--54},
title = {{Methods for evaluating simulated learners: Examples from SimStudent}},
volume = {1432},
year = {2015}
}
@article{Nedungadi2015,
abstract = {An Intelligent Tutoring System (ITS) supplements traditional learning methods and is used for personalized learning purposes that range from exploring simple examples to understanding intricate problems. The Bayesian Knowledge Tracing (BKT) model is an established method for student modeling. A recent enhancement to the BKT model is the BKT-PPS (Prior Per Student) which introduces a prior learnt for each student. Although this method demonstrates improved prediction results compared to the others, there are several aspects that limit its usefulness; (a) for a student, the prior learning is common for all skills, however in reality, it varies for each skill (b) Different students have varying learning capabilities; therefore these students cannot be considered as a homogenous group. In this paper, we aim to improve the prediction of student performance using an enhanced BKT model called the PC-BKT (Personalized {\&} Clustered) with individual priors for each student and skill, and dynamic clustering of students based on changing learning ability. We evaluate the predictions in terms of future performance within ASSISTments intelligent tutoring dataset using over 240,000 log data and show that our models increase the accuracy of student prediction in both the general and the cold start problem.},
author = {Nedungadi, Prema and Remya, M. S.},
doi = {10.1109/FIE.2014.7044200},
file = {:C$\backslash$:/Users/dorot/Desktop/Readings/AL/LR{\_}print{\_}Predicting students' performance on intelligent tutoring system.pdf:pdf},
isbn = {9781479939220},
issn = {15394565},
journal = {Proceedings - Frontiers in Education Conference, FIE},
keywords = {Bayesian Knowledge Tracing,Capability Matrix,Clustering Method,Intelligent Tutoring System(ITS),Personalization},
number = {February},
publisher = {IEEE},
title = {{Predicting students' performance on intelligent tutoring system - Personalized clustered BKT (PC-BKT) model}},
volume = {2015-February},
year = {2015}
}
@article{Yudelson2013,
abstract = {Bayesian Knowledge Tracing (BKT)[1] is a user modeling method extensively used in the area of Intelligent Tutoring Systems. In the standard BKT implementation, there are only skill-specific parameters. However, a large body of research strongly suggests that student-specific variability in the data, when accounted for, could enhance model accuracy [5,6,8]. In this work, we revisit the problem of introducing student-specific parameters into BKT on a larger scale. We show that student-specific parameters lead to a tangible improvement when predicting the data of unseen students, and that parameterizing students' speed of learning is more beneficial than parameterizing a priori knowledge. {\textcopyright} 2013 Springer-Verlag Berlin Heidelberg.},
author = {Yudelson, Michael V. and Koedinger, Kenneth R. and Gordon, Geoffrey J.},
doi = {10.1007/978-3-642-39112-5-18},
file = {:C$\backslash$:/Users/dorot/Desktop/Readings/AL/LR{\_}print{\_}Individualized Bayesian Knowledge Tracing Models (Ken).pdf:pdf},
isbn = {9783642391118},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Bayesian knowledge tracing,Model fitting,Model selection,Student-specific model parameters},
pages = {171--180},
title = {{Individualized bayesian knowledge tracing models}},
volume = {7926 LNAI},
year = {2013}
}
@article{Weitekamp2020,
abstract = {Intelligent tutoring systems (ITSs) have consistently been shown to improve the educational outcomes of students when used alone or combined with traditional instruction. However, building an ITS is a time-consuming process which requires specialized knowledge of existing tools. Extant authoring methods, including the Cognitive Tutor Authoring Tools' (CTAT) example-tracing method and SimStudent's Authoring by Tutoring, use programming-by-demonstration to allow authors to build ITSs more quickly than they could by hand programming with model-tracing. Yet these methods still suffer from long authoring times or difficulty creating complete models. In this study, we demonstrate that Simulated Learners built with the Apprentice Learner (AL) Framework can be combined with a novel interaction design that emphasizes model transparency, input flexibility, and problem solving control to enable authors to achieve greater model completeness in less time than existing authoring methods.},
author = {Weitekamp, Daniel and Harpstead, Erik and Koedinger, Ken R.},
doi = {10.1145/3313831.3376226},
file = {:C$\backslash$:/Users/dorot/Desktop/Readings/AL/An Interaction Design for Machine Teaching to Develop AI Tutors.pdf:pdf},
isbn = {9781450367080},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {intelligent tutoring systems,interaction design,machine teaching,programming-by-demonstration,{\{}simulated learners}},
number = {July},
title = {{An Interaction Design for Machine Teaching to Develop AI Tutors}},
year = {2020}
}
@article{Heffernan2014,
abstract = {The ASSISTments project is an ecosystem of a few hundred teachers, a platform, and researchers working together. Development professionals help train teachers and get teachers to participate in studies. The platform and these teachers help researchers (sometimes explicitly and sometimes implicitly) simply by using content the teacher selects. The platform, hosted by Worcester Polytechnic Institute, allows teachers to write individual ASSISTments (composed of questions with answers and associated hints, solutions, web-based videos, etc.) or to use pre-built ASSISTments, bundle them together in a problem set, and assign these to students. The system gives immediate feedback to students while they are working and provides student-level data to teachers on any assignment. The word "ASSISTments" blends tutoring "assistance" with "assessment" reporting to teachers and students. While originally focused on mathematics, the platform now has content from many other subjects (e.g.; science, English, Statistics, etc.). Due to the large library of mathematics content, however, it is mostly used by math teachers. Over 50,000 students used ASSISTments last school year (2013-4) and this number has been doubling each year for the last 8 years. The platform allows any user, mostly researchers, to create randomized controlled trials in the content, which has helped us use the tool in over 18 published and an equal number of unpublished studies. The data collected by the system has also been used in a few dozen peer-reviewed data mining publications. This paper will not seek to review these publications, but instead we will share why ASSISTments has been successful and what lessons were learned along the way. The first lesson learned was to build a platform for learning sciences, not a product that focused on a math topic. That is, ASSISTments is a tool, not a curriculum. A second lesson learned is expressed by the mantra "Put the teacher in charge, not the computer." This second lesson is about building a flexible system that allows teachers to use the tool in concert with the classroom routine. Once teachers are using the tool they are more likely to want to participate in research studies. These lessons were born from the design decisions about what the platform supports and does not support. In conclusion, goals for the future will be presented.},
author = {Heffernan, Neil T. and Heffernan, Cristina Lindquist},
doi = {10.1007/s40593-014-0024-x},
file = {:C$\backslash$:/Users/dorot/Desktop/Readings/AL/LR{\_}markThreeInARow{\_}The ASSISTments Ecosystem.pdf:pdf},
issn = {15604306},
journal = {International Journal of Artificial Intelligence in Education},
keywords = {Authoring tool,Controlled experiment,Formative assessment,Intelligent tutoring System,RCT},
number = {4},
pages = {470--497},
title = {{The ASSISTments ecosystem: Building a platform that brings scientists and teachers together for minimally invasive research on human learning and teaching}},
volume = {24},
year = {2014}
}
@article{Doroudi2016,
abstract = {How should a wide variety of educational activities be sequenced to maximize student learning? Although some experimental studies have addressed this question, educational data mining methods may be able to evaluate a wider range of possibilities and better handle many simultaneous sequencing constraints. We introduce Sequencing Constraint Violation Analysis (SCOVA): a general method for evaluating alternative activity sequences using existing data. SCOVA can be used to explore many complex sequencing constraints, such as prerequisite relationships, blocking, interleaving, and spiraling. We demonstrate SCOVA on data collected from a fractions intelligent tutoring system (ITS). Some of our findings challenge our initial hypotheses regarding sequencing, illustrating the utility and versatility of the method. The method can also be applied to other learning environments, as long as the available data has substantial variability in students' activity sequences.},
author = {Doroudi, Shayan and Holstein, Kenneth and Aleven, Vincent and Brunskill, Emma},
file = {:C$\backslash$:/Users/dorot/Desktop/Readings/AL/LR-Sequence Matters, But How Exactly A Method for Evaluation Activity Sequences from Data.pdf:pdf},
journal = {Proceedings of the 9th International Conference on Educational Data Mining, EDM 2016},
pages = {70--77},
title = {{Sequence matters, but how exactly? A method for evaluating activity sequences from data}},
year = {2016}
}
@article{Maclellan2015,
abstract = {Problems with many solutions and solution paths are on the frontier of what non-programmers can author with existing tutor authoring tools. Popular approaches such as Example Tracing, which allow authors to build tutors by demonstrating steps directly in the tutor interface. This approach encounters difficulties for problems with more complex solution spaces because the author needs to demonstrate a large number of actions. By using SimStudent, a simulated learner, it is possible to induce general rules from author demonstrations and feedback, enabling efficient support for complexity. In this paper, we present a framework for understanding solution space complexity and analyze the abilities of Example Tracing and SimStudent for authoring problems in an experimental design tutor. We found that both non-programming approaches support authoring of this complex problem. The SimStudent approach is 90{\%} more efficient than Example Tracing, but requires special attention to ensure model completeness. Example Tracing, on the other hand, requires more demonstrations, but reliably arrives at a complete model. In general, Example Tracing's simplicity makes it good for a wide range problems, a reason for why it is currently the most widely used authoring approach. However, SimStudent's improved efficiency makes it a promising non-programmer approach, especially when solution spaces become more complex. Finally, this work demonstrates how simulated learners can be used to efficiently author models for tutoring systems.},
author = {Maclellan, Christopher J. and Harpstead, Erik and Wiese, Eliane Stampfer and Zou, Mengfan and Matsuda, Noboru and Aleven, Vincent and Koedinger, Kenneth R.},
file = {:C$\backslash$:/Users/dorot/Desktop/Readings/AL/Authoring Tutors with Complex Solutions{\_}A Comparative Analysis of Example Tracing and SimStudent.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {Cognitive modeling,Intelligent tutoring systems,Programming-by-demonstration,Tutor authoring},
pages = {35--44},
title = {{Authoring tutors with complex solutions: A comparative analysis of Example Tracing and SimStudent}},
volume = {1432},
year = {2015}
}
@article{MacLellan2014,
abstract = {Authoring Intelligent Tutoring Systems is expensive and time consuming. To reduce costs, the Cognitive Tutor Authoring Tools and the Example-Tracing Tutor paradigm were developed to make the tutor authoring process more efficient. Under this paradigm, tutors are constructed by demonstrating behavior directly in a tutor interface, reducing the need for programming expertise. This paper evaluates the efficiency of authoring a tutor with SimStudent, an extension to the Example-Tracing paradigm that is designed to produce greater generality in less time by induction from past demonstrations and feedback. We found that authoring an algebra tutor in SimStudent is faster than Example-Tracing while maintaining equivalent final model quality. Furthermore, we found that the SimStudent model generalizes beyond the problems that were used to author it. {\textcopyright} 2014 Springer International Publishing Switzerland.},
author = {MacLellan, Christopher J. and Koedinger, Kenneth R. and Matsuda, Noboru},
doi = {10.1007/978-3-319-07221-0_70},
file = {:C$\backslash$:/Users/dorot/Desktop/Readings/AL/Authoring Tutors with SimStudent{\_}An Evaluation of Efficiency and Model Quality.pdf:pdf},
isbn = {9783319072203},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {551--560},
title = {{Authoring tutors with simstudent: An evaluation of efficiency and model quality}},
volume = {8474 LNCS},
year = {2014}
}
@article{Weitekamp2019,
abstract = {Computational models of learning can be powerful tools to test educational technologies, automate the authoring of instructional software, and advance theories of learning. These mechanistic models of learning, which instantiate computational theories of the learning process, are capable of making predictions about learners' performance in instructional technologies given only the technology itself without fitting any parameters to existing learners' data. While these so call "zero-parameter" models have been successful in modeling student learning in intelligent tutoring systems they still show systematic deviation from human learning performance. One deviation stems from the computational models' lack of prior knowledge-all models start off as a blank slate-leading to substantial differences in performance at the first practice opportunity. In this paper, we explore three different strategies for accounting for prior knowledge within computational models of learning and the effect of these strategies on the predictive accuracy of these models.},
author = {Weitekamp, Daniel and Harpstead, Erik and MacLellan, Christopher J. and Rachatasumrit, Napol and Koedinger, Kenneth R.},
file = {:C$\backslash$:/Users/dorot/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Weitekamp et al. - 2019 - Toward near zero-parameter prediction using a computational model of student learning.pdf:pdf},
isbn = {9781733673600},
journal = {EDM 2019 - Proceedings of the 12th International Conference on Educational Data Mining},
number = {Edm},
pages = {456--461},
title = {{Toward near zero-parameter prediction using a computational model of student learning}},
year = {2019}
}
@article{Lee2016,
abstract = {Learning to solve a class of problems can be characterized as a search through a space of hypotheses about the rules for solving these problems. A series of four experiments studied how different learning conditions affected the search among hypotheses about the solution rule for a simple computational problem. Experiment 1 showed that a problem property such as computational difficulty of the rules biased the search process and so affected learning. Experiment 2 examined the impact of examples as instructional tools and found that their effectiveness was determined by whether they uniquely pointed to the correct rule. Experiment 3 compared verbal directions with examples and found that both could guide search. The final experiment tried to improve learning by using more explicit verbal directions or by adding scaffolding to the example. While both manipulations improved learning, learning still took the form of a search through a hypothesis space of possible rules. We describe a model that embodies two assumptions: (1) the instruction can bias the rules participants hypothesize rather than directly be encoded into a rule; (2) participants do not have memory for past wrong hypotheses and are likely to retry them. These assumptions are realized in a Markov model that fits all the data by estimating two sets of probabilities. First, the learning condition induced one set of Start probabilities of trying various rules. Second, should this first hypothesis prove wrong, the learning condition induced a second set of Choice probabilities of considering various rules. These findings broaden our understanding of effective instruction and provide implications for instructional design.},
author = {Lee, Hee Seung and Betts, Shawn and Anderson, John R.},
doi = {10.1111/cogs.12275},
file = {:C$\backslash$:/Users/dorot/Desktop/Readings/AL/LR{\_}Learning Problem-Solving Rules as Search Through a Hypothesis Space​.pdf:pdf},
issn = {15516709},
journal = {Cognitive Science},
keywords = {Examples,Hypothesis testing,Markov processes,Problem solving,Search space,Verbal direction},
number = {5},
pages = {1036--1079},
pmid = {26292648},
title = {{Learning Problem-Solving Rules as Search Through a Hypothesis Space}},
volume = {40},
year = {2016}
}
@article{Doroudi2019,
abstract = {Adaptive educational technologies have the capacity to meet the needs of individual students in theory, but in some cases, the degree of personalization might be less than desired, which could lead to inequitable outcomes for students. In this paper, we use simulations to demonstrate that while knowledge tracing algorithms are substantially more equitable than giving all students the same amount of practice, such algorithms can still be inequitable when they rely on inaccurate models. This can arise as a result of two factors: (1) using student models that are fit to aggregate populations of students, and (2) using student models that make incorrect assumptions about student learning. In particular, we demonstrate that both the Bayesian knowledge tracing algorithm and the N-Consecutive Correct Responses heuristic are susceptible to these concerns, but that knowledge tracing with the additive factor model may be more equitable. The broader message of this paper is that when designing learning analytics algorithms, we need to explicitly consider whether the algorithms act fairly with respect to different populations of students, and if not, how we can make our algorithms more equitable.},
author = {Doroudi, Shayan and Brunskill, Emma},
doi = {10.1145/3303772.3303838},
file = {:C$\backslash$:/Users/dorot/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Doroudi, Brunskill - 2019 - Fairer but not fair enough on the equitability of knowledge tracing.pdf:pdf},
isbn = {9781450362566},
journal = {ACM International Conference Proceeding Series},
keywords = {Equity,Fairness,Knowledge tracing,Model misspecification},
pages = {335--339},
title = {{Fairer but not fair enough on the equitability of knowledge tracing}},
year = {2019}
}
@article{Tenison2014,
abstract = {Education research has identified strategic flexibility as an important aspect of math proficiency and learning. This aspect of student learning has been largely ignored by Intelligent Tutoring Systems (ITSs). In the current study, we demonstrate how Hidden Markov Modeling can be used to identify groups of students who use similar strategies during tutoring and relate these findings to a measure of strategic flexibility. We use these results to explore how strategy use is expressed in an ITS and consider how tutoring systems could integrate a measure of strategy use to improve learning. {\textcopyright} 2014 Springer International Publishing Switzerland.},
author = {Tenison, Caitlin and MacLellan, Christopher J.},
doi = {10.1007/978-3-319-07221-0_58},
file = {:C$\backslash$:/Users/dorot/Desktop/Readings/AL/Modeling Strategy Use in an Intelligent Tutoring System{\_}Implications for Strategic Flexibility.pdf:pdf},
isbn = {9783319072203},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {466--475},
title = {{Modeling strategy use in an intelligent tutoring system: Implications for strategic flexibility}},
volume = {8474 LNCS},
year = {2014}
}
@article{Maclellan,
abstract = {Understanding the nature of human intelligence and developing intelligent agents capable of mod-eling humans are fundamental goals of cognitive systems research. Prior work modeling human problem solving has explored how hand-constructed domain models (e.g., production-rule models) can be used to explain human behavior. Typically, these models account for how humans improve their problem-solving performance given practice (i.e., speed-up learning), but they do not account for how humans acquire initial domain models. One approach that humans use to acquire knowledge in a new domain is apprenticeship learning, or learning from demonstrations and feedback from an expert. In the current work, I formalize the apprenticeship learning task for digital learning environments and present the Apprentice Learner Architecture, which provides a framework for building models of apprenticeship learning that align with this task formalization. Next, I briefly review how this model can be used to simulate and predicting human behavior in intelligent tutors. Finally, I conclude with directions for future work.},
author = {Maclellan, Christopher James},
file = {:C$\backslash$:/Users/dorot/Desktop/Readings/AL/LR{\_}print{\_}Apprentice Learner Architecture{\_}A framework for modeling human learning from demonstrations and feedback in digital environments.pdf:pdf},
pages = {1--6},
title = {{Apprentice Learner Architecture: A framework for modeling human learning from demonstrations and feedback in digital environments}}
}
@article{Agarwal2018,
abstract = {One of the key benefits that Bayesian Knowledge Tracing (BKT) offers compared to many competing student modelling paradigms is that its parameters are meaningful and interpretable. These parameters have been used to answer basic research questions and identify content in need of iterative improvement (due to, for instance, low learning or high slip rates). However, a core challenge to the interpretation of BKT parameters is that several combinations of BKT parameters can often fit the same data comparably well. Even if, as some have argued, BKT is not truly non-identifiable, in practice highly different parameters with comparable goodness are often found using modern BKT fitting packages. These parameter sets can have highly divergent values for guess and slip. Several approaches have been proposed but none of those have yet led to fully stable and trustworthy parameter estimates. In this work, we propose a new iterative method based on contextual guess and slip estimation that converges to stable estimates for skill-level guess and slip parameters. This method alternates between calculating contextual estimates of guess and slip and estimating skill-level parameters, iterating until convergence. Thus, it produces a more stable set of parameters that can be more confidently used in analyzing content efficacy.},
author = {Agarwal, Deepak and Babel, Nishant and Baker, Ryan S.},
file = {:C$\backslash$:/Users/dorot/Desktop/Readings/AL/Contextual Derivation of Stable BKT parameters for Analyzing Content Efficacy.pdf:pdf},
journal = {Proceedings of the 11th International Conference on Educational Data Mining, EDM 2018},
keywords = {Bayesian Knowledge Tracing,Brute Force BKT Model,Content efficacy,Contextual guess slip,EDM},
title = {{Contextual derivation of stable BKT parameters for analyzing content efficacy}},
year = {2018}
}
@article{Fancsali2013,
abstract = {We briefly describe three approaches to simulating students to develop and improve intelligent tutoring systems. We review recent work with simulated student data based on simple probabilistic models that provides important insight into practical decisions made in the deployment of Cognitive Tutor software, focusing specifically on aspects of mastery learning in Bayesian Knowledge Tracing and learning curve analysis to improve cognitive (skill) models. We provide a new simulation approach that builds on earlier efforts to better visualize aggregate learning curves.},
author = {Fancsali, Stephen E. and Nixon, Tristan and Vuong, Annalies and Ritter, Steven},
file = {:C$\backslash$:/Users/dorot/Desktop/Readings/AL/LR{\_}print{\_}Simulated Students, Mastery Learning, and Improved Learning Curves for Real-World Cognitive Tutors (Carnegie Learning).pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {Cognitive Tutor,Knowledge tracing,Learning curves,Mastery learning,Simulated students,Simulation,Student modeling},
pages = {11--20},
title = {{Simulated students, mastery learning, and improved learning curves for real-world cognitive tutors}},
volume = {1009},
year = {2013}
}
@article{Matsuda2010,
abstract = {The purpose of the current study is to test whether we could create a system where students can learn by teaching a live machine-learning agent, called SimStudent. SimStudent is a computer agent that interactively learns cognitive skills through its own tutored-problem solving experience. We have developed a game-like learning environment where students learn algebra equations by tutoring SimStudent. While Simulated Students, Teachable Agents and Learning Companion systems have been created, our study is unique that it genuinely learns skills from student input. This paper describes the overview of the learning environment and some results from an evaluation study. The study showed that after tutoring SimStudent, the students improved their performance on equation solving. The number of correct answers on the error detection items was also significantly improved. On average students spent 70.0 minutes on tutoring SimStudent and used an average of 15 problems for tutoring. {\textcopyright} Springer-Verlag Berlin Heidelberg 2010.},
author = {Matsuda, Noboru and Keiser, Victoria and Raizada, Rohan and Tu, Arthur and Stylianides, Gabriel and Cohen, William W. and Koedinger, Kenneth R.},
doi = {10.1007/978-3-642-13388-6_36},
file = {:C$\backslash$:/Users/dorot/Desktop/Readings/AL/SimSt-ITS2010-03.pdf:pdf},
isbn = {3642133878},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Algebra equation solving,Learning by teaching,Machine learning,Simstudent,Tutor-learning effect},
number = {PART 1},
pages = {317--326},
title = {{Learning by teaching SimStudent: Technical accomplishments and an initial use with students}},
volume = {6094 LNCS},
year = {2010}
}
@article{Fancsali2013a,
abstract = {By implementing mastery learning, intelligent tutoring systems aim to present students with exactly the amount of instruction they need to master a concept. In practice, determination of mastery is imperfect. Student knowledge must be inferred from performance, and performance does not always follow knowledge. A standard method is to set a threshold for mastery, representing a level of certainty that the student has attained mastery. Tutors can make two types of errors when assessing student knowledge: (1) false positives, in which a student without knowledge is judged to have mastered a skill, and (2) false negatives, in which a student is presented with additional practice opportunities after acquiring knowledge. Viewed from this perspective, the mastery threshold can be viewed as a parameter that controls the relative frequency of false negatives and false positives. In this paper, we provide a framework for understanding the role of the mastery threshold in Bayesian Knowledge Tracing and use simulations to model the effects of setting different thresholds under different best and worst-case skill modeling assumptions.},
author = {Fancsali, Stephen E. and Nixon, Tristan and Ritter, Steven},
file = {:C$\backslash$:/Users/dorot/Desktop/Readings/AL/LR{\_}print{\_}Optimal and Worst-Case Performance of Mastery Learning Assessment with Bayesian Knowledge Tracing.pdf:pdf},
isbn = {9780983952527},
journal = {Proceedings of the 6th International Conference on Educational Data Mining, EDM 2013},
keywords = {Cognitive Tutor,Intelligent tutoring systems,Knowledge tracing,Mastery learning,Student modeling},
title = {{Optimal and worst-case performance of mastery learning assessment with Bayesian knowledge tracing}},
year = {2013}
}
@article{Rollinson2015,
abstract = {At their core, Intelligent Tutoring Systems consist of a student model and a policy. The student model captures the state of the student and the policy uses the student model to individualize instruction. Policies require different properties from the student model. For example, a mastery threshold policy requires the student model to have a way to quantify whether the student has mastered a skill. A large amount of work has been done on building student models that can predict student performance on the next question. In this paper, we leverage this prior work with a new whento-stop policy that is compatible with any such predictive student model. Our results suggest that, when employed as part of our new predictive similarity policy, student models with similar predictive accuracies can suggest that substantially different amounts of practice are necessary. This suggests that predictive accuracy may not be a sufficient metric by itself when choosing which student model to use in intelligent tutoring systems.},
author = {Rollinson, Joseph and Brunskill, Emma},
file = {:C$\backslash$:/Users/dorot/Desktop/Readings/AL/ED560516.pdf:pdf},
isbn = {978-84-606-9425-0},
journal = {Proceeding of the 8th International Conference on Educational Data Mining, EDM15},
pages = {179--186},
title = {{From Predictive Models to Instructional Policies}},
url = {http://www.educationaldatamining.org/EDM2015/uploads/papers/paper{\_}33.pdf},
year = {2015}
}
@misc{,
file = {:C$\backslash$:/Users/dorot/Desktop/Readings/AL/EDM2010{\_}presentation.pptx:pptx},
title = {{EDM2010{\_}presentation}}
}
@article{Baker2011,
abstract = {Over the last decades, there have been a rich variety of approaches towards modeling student knowledge and skill within interactive learning environments. There have recently been several empirical comparisons as to which types of student models are better at predicting future performance, both within and outside of the interactive learning environment. However, these comparisons have produced contradictory results. Within this paper, we examine whether ensemble methods, which integrate multiple models, can produce prediction results comparable to or better than the best of nine student modeling frameworks, taken individually. We ensemble model predictions within a Cognitive Tutor for Genetics, at the level of predicting knowledge action-byaction within the tutor. We evaluate the predictions in terms of future performance within the tutor and on a paper post-test. Within this data set, we do not find evidence that ensembles of models are significantly better. Ensembles of models perform comparably to or slightly better than the best individual models, at predicting future performance within the tutor software. However, the ensembles of models perform marginally significantly worse than the best individual models, at predicting post-test performance. {\textcopyright} 2011 Springer-Verlag.},
author = {Baker, Ryan S.J.D. and Pardos, Zachary A. and Gowda, Sujith M. and Nooraei, Bahador B. and Heffernan, Neil T.},
doi = {10.1007/978-3-642-22362-4_2},
file = {:C$\backslash$:/Users/dorot/Desktop/Readings/AL/UMAP2011{\_}ensemble.pdf:pdf},
isbn = {9783642223617},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Bayesian Knowledge-Tracing,Cognitive Tutor,Performance Factors Analysis,ensemble methods,student modeling},
pages = {13--24},
title = {{Ensembling predictions of student knowledge within intelligent tutoring systems}},
volume = {6787 LNCS},
year = {2011}
}
@article{Weitekamp2020a,
abstract = {Simulated learners represent computational theories of human learning that can be used to evaluate educational technologies, provide practice opportunities for teachers, and advance our theoretical understanding of human learning. A key challenge in working with simulated learners is evaluating the accuracy of the simulation compared to the behavior of real human students. One way this evaluation is done is by comparing the error-rate learning curves from a population of human learners and a corresponding set of simulated learners. In this paper, we argue that this approach misses an opportunity to more accurately capture nuances in learning by treating all errors as the same. We present a simulated learner system, the Apprentice Learner (AL) Architecture, and use this more nuanced evaluation to demonstrate ways in which it does and does not explain and accurately predict student learning in terms of the reduction of different kinds of errors over time as it learns, as human students do, from an Intelligent Tutoring System (ITS).},
author = {Weitekamp, Daniel and Ye, Zihuiwen and Rachatasumrit, Napol and Harpstead, Erik and Koedinger, Kenneth},
doi = {10.1007/978-3-030-52237-7_47},
file = {:C$\backslash$:/Users/dorot/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Weitekamp et al. - 2020 - Investigating differential error types between human and simulated learners.pdf:pdf},
isbn = {9783030522360},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Apprentice Learner,Learning curves,Simulated learners},
pages = {586--597},
title = {{Investigating differential error types between human and simulated learners}},
volume = {12163 LNAI},
year = {2020}
}
@misc{Corbett1995,
abstract = {This paper describes an effort to model students' changing knowledge state during skill acquisition.  Students in this research are learning to write short programs with the ACT Programming Tutor (APT).  APT is constructed around a production rule cognitive model of programming knowledge, called the *ideal student model*.  This model allows the tutor to solve exercises along with the student and provide assistance as necessary.  As the student works, the tutor also maintains an estimate of the probability that the student has learned each of the rules in the ideal model, in a process called *knowledge tracing*.  The tutor presents an individualized sequence of exercises to the student based on these probability estimates until the student has 'mastered' each rule.  The programming tutor, cognitive model and learning and performance assumptions are described.  A series of studies is reviewed that examine the empirical validity of knowledge tracing and has led to modificiations in the process.  Currently the model is quite successful in predicting test performance.  Further modifications in the modeling process are discussed that may improve performance levels.},
author = {Corbett, A T and Anderson, John R},
booktitle = {User modeling and user-adapted interaction},
file = {:C$\backslash$:/Users/dorot/Desktop/Readings/AL/LR-Knowledge Tracing{\_}Modeling the Acquisition of Procedural Knowledge.pdf:pdf},
keywords = {empirical validity,individual differences,intelligent tutoring systems,learning,mastery learning,procedural knolwedge,student modeling},
number = {4},
pages = {253--278},
title = {{Knowledge Tracing }},
url = {http://www.springerlink.com/index/M50H664760426738.pdf{\%}5Cnpapers3://publication/livfe/id/110900},
volume = {4},
year = {1995}
}
@article{MacLellan2015,
abstract = {Additive Factors Model (AFM) and Performance Factors Analysis (PFA) are two popular models of student learning that employ logistic regression to estimate parameters and predict performance. This is in contrast to Bayesian Knowledge Tracing (BKT) which uses a Hidden Markov Model formalism. While all three models tend to make similar predictions, they differ in their parameterization of student learning. One key difference is that BKT has parameters for the slipping rates of learned skills, whereas the logistic models do not. Thus, the logistic models assume that as students get more practice their probability of correctly answering monotonically converges to 100{\%}, whereas BKT allows monotonic convergence to lower probabilities. In this paper, we present a novel modification of logistic regression that allows it to account for situations resulting in false negative student actions (e.g., slipping on known skills). We apply this new regression approach to create two new methods AFM+Slip and PFA+Slip and compare the performance of these new models to traditional AFM, PFA, and BKT. We find that across five datasets the new slipping models have the highest accuracy on 10-fold cross validation. We also find evidence that the slip parameters better enable the logistic models to fit steep learning rates, rather than better fitting the tail of learning curves as we expected. Lastly, we explore the use of high slip values as an indicator of skills that might benefit from skill label refinement. We find that after refining the skill model for one dataset using this approach the traditional model fit improved to be on par with the slip model.},
author = {MacLellan, Chr istopher J. and Liu, Ran and Koedinger, Kenneth R.},
file = {:C$\backslash$:/Users/dorot/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/MacLellan, Liu, Koedinger - 2015 - Accounting for Slipping and Other False Negatives in Logistic Models of Student Learning.pdf:pdf},
journal = {Proceeding of the 8th International Conference on Educational Data Mining, EDM15},
keywords = {addi-,cognitive modeling,statistical models of learning},
pages = {53--60},
title = {{Accounting for Slipping and Other False Negatives in Logistic Models of Student Learning}},
year = {2015}
}
@article{Yudelson2013a,
abstract = {Educational Data Mining researchers use various prediction metrics for model selection. Often the improvements one model makes over another, while statistically reliable, seem small. The field has been lacking a metric that informs us on how much practical impact a model improvement may have on student learning efficiency and outcomes. We propose a metric that indicates how much wasted practice can be avoided (increasing efficiency) and extra practice would be added (increasing outcomes) by using a more accurate model. We show that learning can be improved by 15-22{\%} when using machine-discovered skill model improvements across four datasets and by 7-11{\%} by adding individual student estimates to Bayesian Knowledge Tracing.},
author = {Yudelson, Michael V. and Koedinger, Kenneth R.},
file = {:C$\backslash$:/Users/dorot/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yudelson, Koedinger - 2013 - Estimating the benefits of student model improvements on a substantive scale.pdf:pdf},
isbn = {9780983952527},
journal = {Proceedings of the 6th International Conference on Educational Data Mining, EDM 2013},
title = {{Estimating the benefits of student model improvements on a substantive scale}},
year = {2013}
}
@article{Xiong2016,
abstract = {Over the last couple of decades, there have been a large variety of approaches towards modeling student knowledge within intelligent tutoring systems. With the booming development of deep learning and large-scale artificial neural networks, there have been empirical successes in a number of machine learning and data mining applications, including student knowledge modeling. Deep Knowledge Tracing (DKT), a pioneer algorithm that utilizes recurrent neural networks to model student learning, reports substantial improvements in prediction performance. To help the EDM community better understand the promising techniques of deep learning, we examine DKT alongside two well-studied models for knowledge modeling, PFA and BKT. In addition to sharing a primer on the internal computational structures of DKT, we also report on potential issues that arise from data formatting. We take steps to reproduce the experiments of Deep Knowledge Tracing by implementing a DKT algorithm using Google's TensorFlow framework; we also reproduce similar results on new datasets. We determine that the DKT findings don't hold an overall edge when compared to the PFA model, when applied to properly prepared datasets that are limited to main (i.e. non-scaffolding) questions. More importantly, during the investigation of DKT, we not only discovered a data quality issue in a public available data set, but we also detected a vulnerability of DKT at how it handles multiple skill sequences.},
author = {Xiong, Xiaolu and Zhao, Siyuan and {Van Inwegen}, Eric G. and Beck, Joseph E.},
file = {:C$\backslash$:/Users/dorot/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xiong et al. - 2016 - Going deeper with deep knowledge tracing.pdf:pdf},
journal = {Proceedings of the 9th International Conference on Educational Data Mining, EDM 2016},
keywords = {Data quality,Deep learning,Knowledge tracing,Performance factors analysis,Recurrent neural networks,Student modeling},
pages = {545--550},
title = {{Going deeper with deep knowledge tracing}},
year = {2016}
}
@article{Rollinson2015a,
abstract = {At their core, Intelligent Tutoring Systems consist of a student model and a policy. The student model captures the state of the student and the policy uses the student model to individualize instruction. Policies require different properties from the student model. For example, a mastery threshold policy requires the student model to have a way to quantify whether the student has mastered a skill. A large amount of work has been done on building student models that can predict student performance on the next question. In this paper, we leverage this prior work with a new whento-stop policy that is compatible with any such predictive student model. Our results suggest that, when employed as part of our new predictive similarity policy, student models with similar predictive accuracies can suggest that substantially different amounts of practice are necessary. This suggests that predictive accuracy may not be a sufficient metric by itself when choosing which student model to use in intelligent tutoring systems.},
author = {Rollinson, Joseph and Brunskill, Emma},
file = {:C$\backslash$:/Users/dorot/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rollinson, Brunskill - 2015 - From Predictive Models to Instructional Policies.pdf:pdf},
isbn = {978-84-606-9425-0},
journal = {Proceeding of the 8th International Conference on Educational Data Mining, EDM15},
pages = {179--186},
title = {{From Predictive Models to Instructional Policies}},
url = {http://www.educationaldatamining.org/EDM2015/uploads/papers/paper{\_}33.pdf},
year = {2015}
}
@article{MacLellan2016,
abstract = {While Educational Data Mining research has traditionally emphasized the practical aspects of learner modeling, such as predictive modeling, estimating students knowledge, and informing adaptive instruction, in the current study, we argue that Educational Data Mining can also be used to test and improve our fundamental theories of human learning. Using the Apprentice Learner architecture, a computational theory of learning capable of simulating human behavior in interactive learning environments, we generate two models that embody alternative theories of human learning: (1) that humans perfectly recall previous training during learning and (2) that humans only recall a limited window of experience. We evaluate which of these models is better supported by data from two fractions tutoring systems. In general, we find that the model with a complete memory better fits the data than a model recalling only the previous training experience (data-drive theory development). Additionally, we demonstrate that both models are able to predict student performances, as well as, reproduce the main effects of an experimental paradigm without being trained on student data (theory-driven prediction). These results demonstrate how the Apprentice Learner architecture can be used to close the loop between learning theory and educational data.},
author = {MacLellan, Christopher J. and Harpstead, Erik and Patel, Rony and Koedinger, Kenneth R.},
file = {:C$\backslash$:/Users/dorot/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/MacLellan et al. - 2016 - The apprentice learner architecture Closing the loop between learning theory and educational data.pdf:pdf},
journal = {Proceedings of the 9th International Conference on Educational Data Mining, EDM 2016},
pages = {151--158},
title = {{The apprentice learner architecture: Closing the loop between learning theory and educational data}},
year = {2016}
}
@article{Maclellan2020,
abstract = {A key challenge for human performance optimization designers is cost effectively evaluating different interventions. Typically, A/B experiments are used to evaluate interventions, but running these experiments is costly. We explore how computational models of learning can support designers in causally reasoning about alternative interventions for a fractions tutor. We present an approach for automatically tuning models to specific individuals and show that these individualized models make better predictions than generic models. Next, we apply these individualized models to generate counterfactual predictions for how two students (a high and a low-performing student) will respond to three different fractions training interventions. Our model makes predictions that align with previous human findings as well as testable predictions that might be evaluated with future human experiments.},
author = {Maclellan, Christopher James and Maclellan, Christopher and Edu, Drexel and Stowers, Kimberly},
file = {:C$\backslash$:/Users/dorot/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maclellan et al. - 2020 - Optimizing Human Performance using Individualized Computational Models of Learning.pdf:pdf},
journal = {Advances in Cognitive Systems},
number = {2013},
pages = {1--6},
title = {{Optimizing Human Performance using Individualized Computational Models of Learning}},
year = {2020}
}
@article{Maclellan2020a,
abstract = {A key challenge for human performance optimization designers is cost effectively evaluating different interventions. Typically, A/B experiments are used to evaluate interventions, but running these experiments is costly. We explore how computational models of learning can support designers in causally reasoning about alternative interventions for a fractions tutor. We present an approach for automatically tuning models to specific individuals and show that these individualized models make better predictions than generic models. Next, we apply these individualized models to generate counterfactual predictions for how two students (a high and a low-performing student) will respond to three different fractions training interventions. Our model makes predictions that align with previous human findings as well as testable predictions that might be evaluated with future human experiments.},
author = {Maclellan, Christopher James and Maclellan, Christopher and Edu, Drexel and Stowers, Kimberly},
file = {:C$\backslash$:/Users/dorot/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maclellan et al. - Unknown - Optimizing Human Performance using Individualized Computational Models of Learning.pdf:pdf},
journal = {Advances in Cognitive Systems},
number = {2013},
pages = {1--6},
title = {{Optimizing Human Performance using Individualized Computational Models of Learning}},
year = {2020}
}
